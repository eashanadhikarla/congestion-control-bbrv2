{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. bost-dtn | Throughput Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "\n",
    "# rootdir = \"/Users/eashan22/Desktop/Internship 2021/bbrv2/Brian's Project/bost-dtn\"\n",
    "# fileList = []\n",
    "# c = 0\n",
    "\n",
    "# for root, directories, files in os.walk(rootdir):\n",
    "#     for file in files:\n",
    "#         if file.endswith(\"json\"):\n",
    "#             # print(os.path.join(root,file))\n",
    "#             fileList.append(os.path.join(root,file))\n",
    "#             c+=1\n",
    "\n",
    "# print(f\"Total files: {c}\")\n",
    "\n",
    "# cubic, bbr2, bbr = [], [], []\n",
    "# cubic_, bbr2_, bbr_ = [], [], []\n",
    "# key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "# for i,f in enumerate(fileList):\n",
    "#     try:\n",
    "#         path = Path(f)\n",
    "#         # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "#         data = [json.loads(line) for line in open(f, 'r')]\n",
    "\n",
    "#         for i in range(len(data)):\n",
    "#             if key1 in data[i].keys():\n",
    "#                 if data[i]['cubic_data_segs']!=0:\n",
    "#                     cubic.append(data[i]['cubic_data_segs'])\n",
    "#                     cubic_.append(( path.parent.name, data[i]['cubic_data_segs']) )\n",
    "#                 elif data[i]['bbr2_data_segs']!=0:\n",
    "#                     bbr2.append(data[i]['bbr2_data_segs'])\n",
    "#                     bbr2_.append(( path.parent.name, data[i]['bbr2_data_segs']) )\n",
    "#                 elif data[i]['bbr_data_segs']!=0:\n",
    "#                     bbr.append(data[i]['bbr_data_segs'])\n",
    "#                     bbr_.append(( path.parent.name, data[i]['bbr_data_segs']) )\n",
    "\n",
    "#     except:\n",
    "#         print(f\"Index: {i}, Parent: {path.parent.name}\")\n",
    "\n",
    "# print(f\"cubic:{len(cubic):^5}| bbr2:{len(bbr2):^5}| bbr{len(bbr):^5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cubic_mean  = np.mean(cubic) \n",
    "# cubic_mean2 = sum([x[1] for x in cubic_])/len(cubic_)\n",
    "\n",
    "# bbr2_mean   = np.mean(bbr2) \n",
    "# bbr2_mean2  = sum([y[1] for y in bbr2_ ])/len(bbr2_)\n",
    "\n",
    "# bbr_mean    = np.mean(bbr)\n",
    "# bbr_mean2   = sum([z[1] for z in bbr_  ])/len(bbr_)\n",
    "\n",
    "# cubic_var  = np.var(cubic)\n",
    "# cubic_var2 = sum((x[1]-cubic_mean2)**2 for x in cubic_)/len(cubic_)\n",
    "\n",
    "# bbr2_var  = np.var(bbr2)\n",
    "# bbr2_var2  = sum((x[1]-bbr2_mean2)**2 for x in bbr2_)/len(bbr2_)\n",
    "\n",
    "# bbr_var   = np.var(bbr)\n",
    "# bbr_var2  = sum((x[1]-bbr_mean2)**2 for x in bbr_)/len(bbr_)\n",
    "\n",
    "# print(\"--Mean--\")\n",
    "# print(f\"cubic:{cubic_mean:.3f}\\nbbr2:{bbr2_mean:.3f}\\nbbr:{bbr_mean:.3f}\")\n",
    "# print(\"\\n--Variance--\")\n",
    "# print(f\"cubic:{cubic_var:.3f}\\nbbr2:{bbr2_var:.3f}\\nbbr:{bbr_var:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = [x[0] for x in cubic_] \n",
    "# cubic_unique = list(set(temp))\n",
    "# cubic_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbr_both_p16, both_p16, both_p16_bbr1, cubic_p1 = [], [], [], []\n",
    "\n",
    "# for c in cubic_:\n",
    "#     if c[0] == 'pscheduler_bbr_both_p16':\n",
    "#         bbr_both_p16.append(c[1])\n",
    "\n",
    "#     elif c[0] == 'pscheduler_both_p16':\n",
    "#         both_p16.append(c[1])\n",
    "\n",
    "#     elif c[0] == 'pscheduler_both_p16_bbr1':\n",
    "#         both_p16_bbr1.append(c[1])\n",
    "\n",
    "#     elif c[0] == 'pscheduler_cubic_p1':\n",
    "#         cubic_p1.append(c[1])\n",
    "\n",
    "# bbr_both_p16_mean, bbr_both_p16_var = np.mean(bbr_both_p16), np.var(bbr_both_p16)\n",
    "# print(f\"{bbr_both_p16_mean:.3f}, {bbr_both_p16_var:.3f}\")\n",
    "\n",
    "# both_p16_mean, both_p16_var = np.mean(both_p16), np.var(both_p16)\n",
    "# print(f\"{both_p16_mean:.3f}, {both_p16_var:.3f}\")\n",
    "\n",
    "# both_p16_bbr1_mean, both_p16_bbr1_var = np.mean(both_p16_bbr1), np.var(both_p16_bbr1)\n",
    "# print(f\"{both_p16_bbr1_mean:.3f}, {both_p16_bbr1_var:.3f}\")\n",
    "\n",
    "# cubic_p1_mean, cubic_p1_var = np.mean(cubic_p1), np.mean(cubic_p1)\n",
    "# print(f\"{cubic_p1_mean:.3f}, {cubic_p1_var:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "\n",
    "# rootdir = \"/Users/eashan22/Desktop/Internship 2021/bbrv2/Brian's Project/bost-dtn\"\n",
    "# fileList = []\n",
    "# c = 0\n",
    "\n",
    "# for root, directories, files in os.walk(rootdir):\n",
    "#     for file in files:\n",
    "#         if file.endswith(\"json\"):\n",
    "#             fileList.append(os.path.join(root,file))\n",
    "#             c+=1\n",
    "\n",
    "# print(f\"Total files: {c}\")\n",
    "\n",
    "# cubic, bbr2, bbr = [], [], []\n",
    "# c_throughput, b2_throughput, b_throughput = [], [], []\n",
    "# key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "# for i,f in enumerate(fileList):\n",
    "#     try:\n",
    "#         path = Path(f)\n",
    "#         # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "#         data = [json.loads(line) for line in open(f, 'r')]\n",
    "\n",
    "#         for i in range(len(data)):\n",
    "#             if key1 in data[i].keys():\n",
    "#                 if data[i][key1]!=0:\n",
    "#                     # cubic.append(data[i]['cubic_data_segs'])\n",
    "#                     cubic.append(( path.parent.name, data[i][key1]) )\n",
    "#                     tput_c = (data[i][key1]/(9000*data[i]['p50_rtt']))\n",
    "#                     c_throughput.append( tput_c )\n",
    "\n",
    "#                 elif data[i][key2]!=0:\n",
    "#                     # bbr2.append(data[i][key2])\n",
    "#                     bbr2.append(( path.parent.name, data[i][key2]) )\n",
    "#                     tput_b2 = (data[i][key2]/(9000*data[i]['p50_rtt']))\n",
    "#                     b2_throughput.append( tput_b2 )\n",
    "\n",
    "#                 elif data[i][key3]!=0:\n",
    "#                     # bbr.append(data[i][key3])\n",
    "#                     bbr.append(( path.parent.name, data[i][key3]) )\n",
    "#                     tput_b = (data[i][key3]/(9000*data[i]['p50_rtt']))\n",
    "#                     b_throughput.append( tput_b )\n",
    "\n",
    "#     except:\n",
    "#         print(f\"Index: {i}, Parent: {path.parent.name}\")\n",
    "\n",
    "# print(f\"cubic:{len(cubic):^5}| bbr2:{len(bbr2):^5}| bbr{len(bbr):^5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# path_cubic, cubic_data_seg = [x[0] for x in cubic], [x[1] for x in cubic]\n",
    "# path_bbrv2, bbrv2_data_seg  = [x[0] for x in bbr2], [x[1] for x in bbr2]\n",
    "# path_bbrv, bbrv_data_seg   = [x[0] for x in bbr], [x[1] for x in bbr]\n",
    "\n",
    "# cubic_df = pd.DataFrame({'cubic':cubic_data_seg, 'path_cubic':path_cubic})\n",
    "# bbrv2_df = pd.DataFrame({'bbr2':bbrv2_data_seg, 'path_bbrv2':path_bbrv2})\n",
    "# bbrv_df  = pd.DataFrame({'bbr':bbrv_data_seg, 'path_bbrv':path_bbrv})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cubic_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cubic_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbrv2_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbrv_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall = pd.DataFrame({'mean':[1.327231e+07, 1.066952e+07, 1.147213e+07],\n",
    "#                         'std' :[2.693767e+07, 7.357977e+06, 7.902758e+06],\n",
    "#                         'min' :[1.224500e+04, 1.586098e+06, 1.791172e+06],\n",
    "#                         'max' :[2.627112e+08, 2.660069e+07, 2.746252e+07],\n",
    "#                         '25%' :[3.411543e+06, 6.556117e+06, 6.523948e+06],\n",
    "#                         '50%' :[6.660000e+06, 7.914129e+06, 8.107003e+06],\n",
    "#                         '75%' :[9.332570e+06, 1.385351e+07, 1.489011e+07]\n",
    "#                         })\n",
    "\n",
    "# overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "\n",
    "# plt.hist(cubic_data_seg, color='blue')\n",
    "# plt.hist(bbv2_data_seg, color='red')\n",
    "# plt.hist(bbrv_data_seg, color='green')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bost-dtn Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# rootdir = \"/Users/eashan22/Desktop/Internship 2021/bbrv2/Brian's Project/bost-dtn\"\n",
    "rootdir = \"/home/eadhikarla/Desktop/Brian's Project/bost-dtn\"\n",
    "\n",
    "def traverse(path):\n",
    "    fileList = []\n",
    "    c = 0\n",
    "    for root, directories, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\"json\"):\n",
    "                fileList.append(os.path.join(root,file))\n",
    "                c+=1\n",
    "    print(f\"Total files: {c}\")\n",
    "    return fileList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==========================================\n",
    "## bost-dtn (2021-06-09:14:35)\n",
    "### 1. bbr2_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 105\n",
      "float division by zero\n",
      "Mean: 2.0925770761598767e-07  | Variance: 8.906376684440232e-06\n"
     ]
    }
   ],
   "source": [
    "pscheduler_bbr2_p1  = os.path.join(rootdir, \"2021-06-09:14:35/pscheduler_bbr2_p1\")\n",
    "filenames = traverse(pscheduler_bbr2_p1)\n",
    "\n",
    "tput_bbr2_p1 = []\n",
    "key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and i==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key2 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                bbr2_data_seg = data[j][key2]\n",
    "                throughput = (bbr2_data_seg*mss*8)/(end_time-start_time)\n",
    "                tput_bbr2_p1.append( throughput )\n",
    "        # print(f\"Throughput: {(throughput/1e9):.4f} Gbps/sec\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(f\"Mean: {np.mean(tput_bbr2_p1)*9.31e-10}  | Variance: {np.var(tput_bbr2_p1)*9.31e-10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bost-dtn (2021-06-09:14:35)\n",
    "### 2. cubic_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 105\n",
      "float division by zero\n",
      "Mean: 2.269248056668279e-07  | Variance: 1.1042407481302774e-05\n"
     ]
    }
   ],
   "source": [
    "pscheduler_cubic_p1 = os.path.join(rootdir, \"2021-06-09:14:35/pscheduler_cubic_p1\")\n",
    "filenames = traverse(pscheduler_cubic_p1)\n",
    "\n",
    "tput_p1_cubic = []\n",
    "key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and i==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key1 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                bbr2_data_seg = data[j][key1]\n",
    "                throughput = (bbr2_data_seg*mss*8)/(end_time-start_time)\n",
    "                tput_p1_cubic.append( throughput )\n",
    "        # print(f\"Throughput: {(throughput/1e9):.4f} Gbps/sec\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(f\"Mean: {np.mean(tput_p1_cubic)*9.31e-10}  | Variance: {np.var(tput_p1_cubic)*9.31e-10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bost-dtn (2021-06-09:14:35)\n",
    "### 3. both_p16  -----> Fig. 4b. in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 109\n",
      "Cubic Mean: 4.0103  | Cubic Variance: 3747665639.8610\n",
      "Bbrv2 Mean: 3.1851  | Bbrv2 Variance: 1570301724.1536\n"
     ]
    }
   ],
   "source": [
    "pscheduler_both_p16 = os.path.join(rootdir, \"2021-06-09:14:35/pscheduler_both_p16\")\n",
    "filenames = traverse(pscheduler_both_p16)\n",
    "\n",
    "tput_cubic_p16, tput_bbr2_p16 = [], []\n",
    "key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        throughput_cubic, throughput_bbr2, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for i,d in zip(range(len(data)),data):\n",
    "            try:\n",
    "                if \"interval\" in d.keys() and i==0:\n",
    "                    start_time = d['interval']['time'] # Start time of the test\n",
    "                if \"streams\" in d.keys():\n",
    "                    end_time = data[i-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[i-1]['interval']['mss'] # maximum segment size\n",
    "                    \n",
    "                    cubic_data_seg_list, bbr2_data_seg_list = [], []\n",
    "                    for j in range(len(d['streams'])):\n",
    "                        if \"cubic\" in d['streams'][j]['cc']:\n",
    "                            cubic_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                        elif \"bbr2\" in d['streams'][j]['cc']:\n",
    "                            bbr2_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                    \n",
    "                    throughput_cubic = (sum(cubic_data_seg_list)*mss*8)/(end_time-start_time)\n",
    "                    tput_cubic_p16.append( throughput_cubic )\n",
    "\n",
    "                    throughput_bbr2 = (sum(bbr2_data_seg_list)*mss*8)/(end_time-start_time)\n",
    "                    tput_bbr2_p16.append( throughput_bbr2 )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(f\"Cubic Mean: {np.mean(tput_cubic_p16)*9.31e-10:.4f}  | Cubic Variance: {np.var(tput_cubic_p16)*9.31e-10:.4f}\")\n",
    "print(f\"Bbrv2 Mean: {np.mean(tput_bbr2_p16)*9.31e-10:.4f}  | Bbrv2 Variance: {np.var(tput_bbr2_p16)*9.31e-10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==========================================\n",
    "## bost-dtn (2021-06-15:02:12)\n",
    "### 1. bbr2_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 45\n",
      "float division by zero\n",
      "Mean: 2.388578878395818e-07  | Variance: 1.0373092530293932e-05\n"
     ]
    }
   ],
   "source": [
    "pscheduler_bbr2_p1  = os.path.join(rootdir, \"2021-06-15:02:12/pscheduler_bbr2_p1\")\n",
    "filenames = traverse(pscheduler_bbr2_p1)\n",
    "\n",
    "tput_bbr2_p1 = []\n",
    "key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and i==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key2 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                bbr2_data_seg = data[j][key2]\n",
    "                throughput = (bbr2_data_seg*mss*8)/(end_time-start_time)\n",
    "                tput_bbr2_p1.append( throughput )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(f\"Mean: {np.mean(tput_bbr2_p1)*9.31e-10}  | Variance: {np.var(tput_bbr2_p1)*9.31e-10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bost-dtn (2021-06-15:02:12)\n",
    "### 2. cubic_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 47\n",
      "float division by zero\n",
      "Mean: 2.3386923435703782e-07  | Variance: 1.1819421012832947e-05\n"
     ]
    }
   ],
   "source": [
    "pscheduler_cubic_p1 = os.path.join(rootdir, \"2021-06-15:02:12/pscheduler_cubic_p1\")\n",
    "filenames = traverse(pscheduler_cubic_p1)\n",
    "\n",
    "tput_p1_cubic = []\n",
    "key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and i==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key1 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                bbr2_data_seg = data[j][key1]\n",
    "                throughput = (bbr2_data_seg*mss*8)/(end_time-start_time)\n",
    "                tput_p1_cubic.append( throughput )\n",
    "        # print(f\"Throughput: {(throughput/1e9):.4f} Gbps/sec\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(f\"Mean: {np.mean(tput_p1_cubic)*9.31e-10}  | Variance: {np.var(tput_p1_cubic)*9.31e-10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bost-dtn (2021-06-15:02:12)\n",
    "### 3. both_p16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 50\n",
      "Cubic Mean: 2.1981  | Cubic Variance: 3772052442.1765\n",
      "Bbrv2 Mean: 5.4003  | Bbrv2 Variance: 3747894822.6419\n"
     ]
    }
   ],
   "source": [
    "pscheduler_both_p16 = os.path.join(rootdir, \"2021-06-15:02:12/pscheduler_both_p16\")\n",
    "filenames = traverse(pscheduler_both_p16)\n",
    "\n",
    "tput_cubic_p16, tput_bbr2_p16 = [], []\n",
    "key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        throughput_cubic, throughput_bbr2, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for i,d in zip(range(len(data)),data):\n",
    "            try:\n",
    "                if \"interval\" in d.keys() and i==0:\n",
    "                    start_time = d['interval']['time'] # Start time of the test\n",
    "                if \"streams\" in d.keys():\n",
    "                    end_time = data[i-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[i-1]['interval']['mss'] # maximum segment size\n",
    "                    \n",
    "                    cubic_data_seg_list, bbr2_data_seg_list = [], []\n",
    "                    for j in range(len(d['streams'])):\n",
    "                        if \"cubic\" in d['streams'][j]['cc']:\n",
    "                            cubic_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                        elif \"bbr2\" in d['streams'][j]['cc']:\n",
    "                            bbr2_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                    \n",
    "                    throughput_cubic = (sum(cubic_data_seg_list)*mss*8)/(end_time-start_time)\n",
    "                    tput_cubic_p16.append( throughput_cubic )\n",
    "\n",
    "                    throughput_bbr2 = (sum(bbr2_data_seg_list)*mss*8)/(end_time-start_time)\n",
    "                    tput_bbr2_p16.append( throughput_bbr2 )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(f\"Cubic Mean: {np.mean(tput_cubic_p16)*9.31e-10:.4f}  | Cubic Variance: {np.var(tput_cubic_p16)*9.31e-10:.4f}\")\n",
    "print(f\"Bbrv2 Mean: {np.mean(tput_bbr2_p16)*9.31e-10:.4f}  | Bbrv2 Variance: {np.var(tput_bbr2_p16)*9.31e-10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==========================================\n",
    "## bost-dtn (2021-06-16:02:12)\n",
    "### 1. bbr2_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 55\n",
      "float division by zero\n",
      "Mean: 2.202604800607542e-07  | Variance: 1.0475994041066544e-05\n"
     ]
    }
   ],
   "source": [
    "pscheduler_bbr2_p1  = os.path.join(rootdir, \"2021-06-16:02:12/pscheduler_bbr2_p1\")\n",
    "filenames = traverse(pscheduler_bbr2_p1)\n",
    "\n",
    "tput_bbr2_p1 = []\n",
    "key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and i==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key2 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                bbr2_data_seg = data[j][key2]\n",
    "                throughput = (bbr2_data_seg*mss*8)/(end_time-start_time)\n",
    "                tput_bbr2_p1.append( throughput )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(f\"Mean: {np.mean(tput_bbr2_p1)*9.31e-10}  | Variance: {np.var(tput_bbr2_p1)*9.31e-10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bost-dtn (2021-06-16:02:12)\n",
    "### 2. cubic_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 54\n",
      "float division by zero\n",
      "Mean: 2.2086850479781964e-07  | Variance: 1.2285207548054892e-05\n"
     ]
    }
   ],
   "source": [
    "pscheduler_cubic_p1 = os.path.join(rootdir, \"2021-06-16:02:12/pscheduler_cubic_p1\")\n",
    "filenames = traverse(pscheduler_cubic_p1)\n",
    "\n",
    "tput_p1_cubic = []\n",
    "key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and i==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key1 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                bbr2_data_seg = data[j][key1]\n",
    "                throughput = (bbr2_data_seg*mss*8)/(end_time-start_time)\n",
    "                tput_p1_cubic.append( throughput )\n",
    "        # print(f\"Throughput: {(throughput/1e9):.4f} Gbps/sec\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(f\"Mean: {np.mean(tput_p1_cubic)*9.31e-10}  | Variance: {np.var(tput_p1_cubic)*9.31e-10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bost-dtn (2021-06-16:02:12)\n",
    "### 3. both_p16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 51\n",
      "Cubic Mean: 1.6711  | Cubic Variance: 2946501775.2520\n",
      "Bbrv2 Mean: 5.7825  | Bbrv2 Variance: 3714884469.5304\n"
     ]
    }
   ],
   "source": [
    "pscheduler_both_p16 = os.path.join(rootdir, \"2021-06-16:02:12/pscheduler_both_p16\")\n",
    "filenames = traverse(pscheduler_both_p16)\n",
    "\n",
    "tput_cubic_p16, tput_bbr2_p16 = [], []\n",
    "key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        throughput_cubic, throughput_bbr2, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for i,d in zip(range(len(data)),data):\n",
    "            try:\n",
    "                if \"interval\" in d.keys() and i==0:\n",
    "                    start_time = d['interval']['time'] # Start time of the test\n",
    "                if \"streams\" in d.keys():\n",
    "                    end_time = data[i-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[i-1]['interval']['mss'] # maximum segment size\n",
    "                    \n",
    "                    cubic_data_seg_list, bbr2_data_seg_list = [], []\n",
    "                    for j in range(len(d['streams'])):\n",
    "                        if \"cubic\" in d['streams'][j]['cc']:\n",
    "                            cubic_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                        elif \"bbr2\" in d['streams'][j]['cc']:\n",
    "                            bbr2_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                    \n",
    "                    throughput_cubic = (sum(cubic_data_seg_list)*mss*8)/(end_time-start_time)\n",
    "                    tput_cubic_p16.append( throughput_cubic )\n",
    "\n",
    "                    throughput_bbr2 = (sum(bbr2_data_seg_list)*mss*8)/(end_time-start_time)\n",
    "                    tput_bbr2_p16.append( throughput_bbr2 )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(f\"Cubic Mean: {np.mean(tput_cubic_p16)*9.31e-10:.4f}  | Cubic Variance: {np.var(tput_cubic_p16)*9.31e-10:.4f}\")\n",
    "print(f\"Bbrv2 Mean: {np.mean(tput_bbr2_p16)*9.31e-10:.4f}  | Bbrv2 Variance: {np.var(tput_bbr2_p16)*9.31e-10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buffer Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6e2000e74d83ab38a7a73c8353776f8595191be383670aed524d2a15b88663d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}