{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1. bost-dtn | Throughput Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# import os\n",
    "# import json\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "\n",
    "# rootdir = \"/Users/eashan22/Desktop/Internship 2021/bbrv2/Brian's Project/bost-dtn\"\n",
    "# fileList = []\n",
    "# c = 0\n",
    "\n",
    "# for root, directories, files in os.walk(rootdir):\n",
    "#     for file in files:\n",
    "#         if file.endswith(\"json\"):\n",
    "#             # print(os.path.join(root,file))\n",
    "#             fileList.append(os.path.join(root,file))\n",
    "#             c+=1\n",
    "\n",
    "# print(f\"Total files: {c}\")\n",
    "\n",
    "# cubic, bbr2, bbr = [], [], []\n",
    "# cubic_, bbr2_, bbr_ = [], [], []\n",
    "# key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "# for i,f in enumerate(fileList):\n",
    "#     try:\n",
    "#         path = Path(f)\n",
    "#         # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "#         data = [json.loads(line) for line in open(f, 'r')]\n",
    "\n",
    "#         for i in range(len(data)):\n",
    "#             if key1 in data[i].keys():\n",
    "#                 if data[i]['cubic_data_segs']!=0:\n",
    "#                     cubic.append(data[i]['cubic_data_segs'])\n",
    "#                     cubic_.append(( path.parent.name, data[i]['cubic_data_segs']) )\n",
    "#                 elif data[i]['bbr2_data_segs']!=0:\n",
    "#                     bbr2.append(data[i]['bbr2_data_segs'])\n",
    "#                     bbr2_.append(( path.parent.name, data[i]['bbr2_data_segs']) )\n",
    "#                 elif data[i]['bbr_data_segs']!=0:\n",
    "#                     bbr.append(data[i]['bbr_data_segs'])\n",
    "#                     bbr_.append(( path.parent.name, data[i]['bbr_data_segs']) )\n",
    "\n",
    "#     except:\n",
    "#         print(f\"Index: {i}, Parent: {path.parent.name}\")\n",
    "\n",
    "# print(f\"cubic:{len(cubic):^5}| bbr2:{len(bbr2):^5}| bbr{len(bbr):^5}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# cubic_mean  = np.mean(cubic) \n",
    "# cubic_mean2 = sum([x[1] for x in cubic_])/len(cubic_)\n",
    "\n",
    "# bbr2_mean   = np.mean(bbr2) \n",
    "# bbr2_mean2  = sum([y[1] for y in bbr2_ ])/len(bbr2_)\n",
    "\n",
    "# bbr_mean    = np.mean(bbr)\n",
    "# bbr_mean2   = sum([z[1] for z in bbr_  ])/len(bbr_)\n",
    "\n",
    "# cubic_var  = np.var(cubic)\n",
    "# cubic_var2 = sum((x[1]-cubic_mean2)**2 for x in cubic_)/len(cubic_)\n",
    "\n",
    "# bbr2_var  = np.var(bbr2)\n",
    "# bbr2_var2  = sum((x[1]-bbr2_mean2)**2 for x in bbr2_)/len(bbr2_)\n",
    "\n",
    "# bbr_var   = np.var(bbr)\n",
    "# bbr_var2  = sum((x[1]-bbr_mean2)**2 for x in bbr_)/len(bbr_)\n",
    "\n",
    "# print(\"--Mean--\")\n",
    "# print(f\"cubic:{cubic_mean:.3f}\\nbbr2:{bbr2_mean:.3f}\\nbbr:{bbr_mean:.3f}\")\n",
    "# print(\"\\n--Variance--\")\n",
    "# print(f\"cubic:{cubic_var:.3f}\\nbbr2:{bbr2_var:.3f}\\nbbr:{bbr_var:.3f}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# temp = [x[0] for x in cubic_] \n",
    "# cubic_unique = list(set(temp))\n",
    "# cubic_unique"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# bbr_both_p16, both_p16, both_p16_bbr1, cubic_p1 = [], [], [], []\n",
    "\n",
    "# for c in cubic_:\n",
    "#     if c[0] == 'pscheduler_bbr_both_p16':\n",
    "#         bbr_both_p16.append(c[1])\n",
    "\n",
    "#     elif c[0] == 'pscheduler_both_p16':\n",
    "#         both_p16.append(c[1])\n",
    "\n",
    "#     elif c[0] == 'pscheduler_both_p16_bbr1':\n",
    "#         both_p16_bbr1.append(c[1])\n",
    "\n",
    "#     elif c[0] == 'pscheduler_cubic_p1':\n",
    "#         cubic_p1.append(c[1])\n",
    "\n",
    "# bbr_both_p16_mean, bbr_both_p16_var = np.mean(bbr_both_p16), np.var(bbr_both_p16)\n",
    "# print(f\"{bbr_both_p16_mean:.3f}, {bbr_both_p16_var:.3f}\")\n",
    "\n",
    "# both_p16_mean, both_p16_var = np.mean(both_p16), np.var(both_p16)\n",
    "# print(f\"{both_p16_mean:.3f}, {both_p16_var:.3f}\")\n",
    "\n",
    "# both_p16_bbr1_mean, both_p16_bbr1_var = np.mean(both_p16_bbr1), np.var(both_p16_bbr1)\n",
    "# print(f\"{both_p16_bbr1_mean:.3f}, {both_p16_bbr1_var:.3f}\")\n",
    "\n",
    "# cubic_p1_mean, cubic_p1_var = np.mean(cubic_p1), np.mean(cubic_p1)\n",
    "# print(f\"{cubic_p1_mean:.3f}, {cubic_p1_var:.3f}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# import os\n",
    "# import json\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "\n",
    "# rootdir = \"/Users/eashan22/Desktop/Internship 2021/bbrv2/Brian's Project/bost-dtn\"\n",
    "# fileList = []\n",
    "# c = 0\n",
    "\n",
    "# for root, directories, files in os.walk(rootdir):\n",
    "#     for file in files:\n",
    "#         if file.endswith(\"json\"):\n",
    "#             fileList.append(os.path.join(root,file))\n",
    "#             c+=1\n",
    "\n",
    "# print(f\"Total files: {c}\")\n",
    "\n",
    "# cubic, bbr2, bbr = [], [], []\n",
    "# c_throughput, b2_throughput, b_throughput = [], [], []\n",
    "# key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "# for i,f in enumerate(fileList):\n",
    "#     try:\n",
    "#         path = Path(f)\n",
    "#         # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "#         data = [json.loads(line) for line in open(f, 'r')]\n",
    "\n",
    "#         for i in range(len(data)):\n",
    "#             if key1 in data[i].keys():\n",
    "#                 if data[i][key1]!=0:\n",
    "#                     # cubic.append(data[i]['cubic_data_segs'])\n",
    "#                     cubic.append(( path.parent.name, data[i][key1]) )\n",
    "#                     tput_c = (data[i][key1]/(9000*data[i]['p50_rtt']))\n",
    "#                     c_throughput.append( tput_c )\n",
    "\n",
    "#                 elif data[i][key2]!=0:\n",
    "#                     # bbr2.append(data[i][key2])\n",
    "#                     bbr2.append(( path.parent.name, data[i][key2]) )\n",
    "#                     tput_b2 = (data[i][key2]/(9000*data[i]['p50_rtt']))\n",
    "#                     b2_throughput.append( tput_b2 )\n",
    "\n",
    "#                 elif data[i][key3]!=0:\n",
    "#                     # bbr.append(data[i][key3])\n",
    "#                     bbr.append(( path.parent.name, data[i][key3]) )\n",
    "#                     tput_b = (data[i][key3]/(9000*data[i]['p50_rtt']))\n",
    "#                     b_throughput.append( tput_b )\n",
    "\n",
    "#     except:\n",
    "#         print(f\"Index: {i}, Parent: {path.parent.name}\")\n",
    "\n",
    "# print(f\"cubic:{len(cubic):^5}| bbr2:{len(bbr2):^5}| bbr{len(bbr):^5}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# path_cubic, cubic_data_seg = [x[0] for x in cubic], [x[1] for x in cubic]\n",
    "# path_bbrv2, bbrv2_data_seg  = [x[0] for x in bbr2], [x[1] for x in bbr2]\n",
    "# path_bbrv, bbrv_data_seg   = [x[0] for x in bbr], [x[1] for x in bbr]\n",
    "\n",
    "# cubic_df = pd.DataFrame({'cubic':cubic_data_seg, 'path_cubic':path_cubic})\n",
    "# bbrv2_df = pd.DataFrame({'bbr2':bbrv2_data_seg, 'path_bbrv2':path_bbrv2})\n",
    "# bbrv_df  = pd.DataFrame({'bbr':bbrv_data_seg, 'path_bbrv':path_bbrv})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# cubic_df.tail(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# cubic_df.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# bbrv2_df.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# bbrv_df.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# overall = pd.DataFrame({'mean':[1.327231e+07, 1.066952e+07, 1.147213e+07],\n",
    "#                         'std' :[2.693767e+07, 7.357977e+06, 7.902758e+06],\n",
    "#                         'min' :[1.224500e+04, 1.586098e+06, 1.791172e+06],\n",
    "#                         'max' :[2.627112e+08, 2.660069e+07, 2.746252e+07],\n",
    "#                         '25%' :[3.411543e+06, 6.556117e+06, 6.523948e+06],\n",
    "#                         '50%' :[6.660000e+06, 7.914129e+06, 8.107003e+06],\n",
    "#                         '75%' :[9.332570e+06, 1.385351e+07, 1.489011e+07]\n",
    "#                         })\n",
    "\n",
    "# overall"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "\n",
    "# plt.hist(cubic_data_seg, color='blue')\n",
    "# plt.hist(bbv2_data_seg, color='red')\n",
    "# plt.hist(bbrv_data_seg, color='green')\n",
    "\n",
    "# plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bost-dtn Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# rootdir = \"/Users/eashan22/Desktop/Internship 2021/bbrv2/Brian's Project/bost-dtn\"\n",
    "rootdir = \"/home/eadhikarla/Desktop/Brian's Project/bost-dtn\"\n",
    "\n",
    "def traverse(path):\n",
    "    fileList = []\n",
    "    c = 0\n",
    "    for root, directories, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\"json\"):\n",
    "                fileList.append(os.path.join(root,file))\n",
    "                c+=1\n",
    "    print(f\"Total files: {c}\")\n",
    "    return fileList"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "==========================================\n",
    "## bost-dtn (2021-06-09:14:35)\n",
    "### 1. bbr2_p1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "pscheduler_bbr2_p1  = os.path.join(rootdir, \"2021-06-09:14:35/pscheduler_bbr2_p1\")\n",
    "filenames = traverse(pscheduler_bbr2_p1)\n",
    "\n",
    "data_seg = []\n",
    "tput_bbr2_p1 = []\n",
    "key2 = 'bbr2_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and i==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key2 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                bbr2_data_seg = data[j][key2]\n",
    "                data_seg.append(bbr2_data_seg)\n",
    "\n",
    "                throughput = (bbr2_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                tput_bbr2_p1.append( throughput )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "print(\"Throughput\")\n",
    "print(f\"Mean: {np.mean(tput_bbr2_p1)}  | Variance: {np.var(tput_bbr2_p1)}\")\n",
    "print(\"Data Segment\")\n",
    "print(f\"Mean: {np.mean(data_seg):f}  | Variance: {np.var(data_seg):f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 105\n",
      "float division by zero\n",
      "Throughput\n",
      "Mean: 2.2476660323951416e-07  | Variance: 9.566462604124848e-15\n",
      "Data Segment\n",
      "Mean: 10593937.790476  | Variance: 53354882026455.648438\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bost-dtn (2021-06-09:14:35)\n",
    "### 2. cubic_p1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "pscheduler_cubic_p1 = os.path.join(rootdir, \"2021-06-09:14:35/pscheduler_cubic_p1\")\n",
    "filenames = traverse(pscheduler_cubic_p1)\n",
    "\n",
    "data_seg = []\n",
    "tput_p1_cubic = []\n",
    "key1 = 'cubic_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        cubic_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and i==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key1 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                cubic_data_seg = data[j][key1]\n",
    "                data_seg.append(cubic_data_seg)\n",
    "\n",
    "                throughput = (cubic_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                tput_p1_cubic.append( throughput )\n",
    "        # print(f\"Throughput: {(throughput/1e9):.4f} Gbps/sec\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"Mean: {np.mean(tput_p1_cubic)}  | Variance: {np.var(tput_p1_cubic)}\")\n",
    "print(\"Data Segment\")\n",
    "print(f\"Mean: {np.mean(data_seg)}  | Variance: {np.var(data_seg)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 105\n",
      "float division by zero\n",
      "Throughput\n",
      "Mean: 2.437430780524467e-07  | Variance: 1.1860802880024463e-14\n",
      "Data Segment\n",
      "Mean: 11281322.247619048  | Variance: 59836109647663.09\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bost-dtn (2021-06-09:14:35)\n",
    "### 3. both_p16  -----> Fig. 4b. in paper"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "pscheduler_both_p16 = os.path.join(rootdir, \"2021-06-09:14:35/pscheduler_both_p16\")\n",
    "filenames = traverse(pscheduler_both_p16)\n",
    "\n",
    "data_seg_sum_cubic, data_seg_sum_bbr2 = [], []\n",
    "tput_cubic_p16, tput_bbr2_p16 = [], []\n",
    "key1, key2 = 'cubic_data_segs', 'bbr2_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        throughput_cubic, throughput_bbr2, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for i,d in zip(range(len(data)),data):\n",
    "            try:\n",
    "                if \"interval\" in d.keys() and i==0:\n",
    "                    start_time = d['interval']['time'] # Start time of the test\n",
    "                if \"streams\" in d.keys():\n",
    "                    end_time = data[i-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[i-1]['interval']['mss'] # maximum segment size\n",
    "                    \n",
    "                    cubic_data_seg_list, bbr2_data_seg_list = [], []\n",
    "                    for j in range(len(d['streams'])):\n",
    "                        if \"cubic\" in d['streams'][j]['cc']:\n",
    "                            cubic_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                        elif \"bbr2\" in d['streams'][j]['cc']:\n",
    "                            bbr2_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                    \n",
    "                    data_seg_sum_cubic.append( sum(cubic_data_seg_list) )\n",
    "                    throughput_cubic = (sum(cubic_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_cubic_p16.append( throughput_cubic )\n",
    "\n",
    "                    data_seg_sum_bbr2.append( sum(bbr2_data_seg_list) )\n",
    "                    throughput_bbr2 = (sum(bbr2_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_bbr2_p16.append( throughput_bbr2 )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"Cubic Mean: {np.mean(tput_cubic_p16)}  | Cubic Variance: {np.var(tput_cubic_p16)}\")\n",
    "print(f\"Bbrv2 Mean: {np.mean(tput_bbr2_p16)}  | Bbrv2 Variance: {np.var(tput_bbr2_p16)}\")\n",
    "\n",
    "print(\"Data Segment\")\n",
    "print(f\"Cubic Mean: {np.mean(data_seg_sum_cubic)}  | Cubic Variance: {np.var(data_seg_sum_cubic)}\")\n",
    "print(f\"Bbrv2 Mean: {np.mean(data_seg_sum_bbr2)}  | Bbrv2 Variance: {np.var(data_seg_sum_bbr2)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 109\n",
      "Throughput\n",
      "Cubic Mean: 4.307491100831607  | Cubic Variance: 4.025419591687469\n",
      "Bbrv2 Mean: 3.421196237623312  | Bbrv2 Variance: 1.6866828401219744\n",
      "Data Segment\n",
      "Cubic Mean: 19844388.688073393  | Cubic Variance: 1702568119688929.2\n",
      "Bbrv2 Mean: 16160038.23853211  | Bbrv2 Variance: 663028364607013.2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "==========================================\n",
    "## bost-dtn (2021-06-15:02:12)\n",
    "### 1. bbr2_p1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "pscheduler_bbr2_p1  = os.path.join(rootdir, \"2021-06-15:02:12/pscheduler_bbr2_p1\")\n",
    "filenames = traverse(pscheduler_bbr2_p1)\n",
    "\n",
    "data_seg = []\n",
    "tput_bbr2_p1 = []\n",
    "key2 = 'bbr2_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and i==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key2 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                bbr2_data_seg = data[j][key2]\n",
    "                data_seg.append(bbr2_data_seg)\n",
    "\n",
    "                throughput = (bbr2_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                tput_bbr2_p1.append( throughput )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"Mean: {np.mean(tput_bbr2_p1)}  | Variance: {np.var(tput_bbr2_p1)}\")\n",
    "print(\"Data Segment\")\n",
    "print(f\"Mean: {np.mean(data_seg)}  | Variance: {np.var(data_seg)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 45\n",
      "float division by zero\n",
      "Throughput\n",
      "Mean: 2.565605669598086e-07  | Variance: 1.1141882417071894e-14\n",
      "Data Segment\n",
      "Mean: 9662307.777777778  | Variance: 46214551992636.76\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bost-dtn (2021-06-15:02:12)\n",
    "### 2. cubic_p1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "pscheduler_cubic_p1 = os.path.join(rootdir, \"2021-06-15:02:12/pscheduler_cubic_p1\")\n",
    "filenames = traverse(pscheduler_cubic_p1)\n",
    "\n",
    "data_seg = []\n",
    "tput_p1_cubic = []\n",
    "key1 = 'cubic_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        cubic_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and i==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key1 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                cubic_data_seg = data[j][key1]\n",
    "                data_seg.append(cubic_data_seg)\n",
    "\n",
    "                throughput = (cubic_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                tput_p1_cubic.append( throughput )\n",
    "        # print(f\"Throughput: {(throughput/1e9):.4f} Gbps/sec\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"Mean: {np.mean(tput_p1_cubic)}  | Variance: {np.var(tput_p1_cubic)}\")\n",
    "print(\"Data Segment\")\n",
    "print(f\"Mean: {np.mean(data_seg)}  | Variance: {np.var(data_seg)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 47\n",
      "float division by zero\n",
      "Throughput\n",
      "Mean: 2.512021851310826e-07  | Variance: 1.2695403880593927e-14\n",
      "Data Segment\n",
      "Mean: 9722667.617021276  | Variance: 47272544227031.08\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bost-dtn (2021-06-15:02:12)\n",
    "### 3. both_p16"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "pscheduler_both_p16 = os.path.join(rootdir, \"2021-06-15:02:12/pscheduler_both_p16\")\n",
    "filenames = traverse(pscheduler_both_p16)\n",
    "\n",
    "data_seg_sum_cubic, data_seg_sum_bbr2 = [], []\n",
    "tput_cubic_p16, tput_bbr2_p16 = [], []\n",
    "key1, key2 = 'cubic_data_segs', 'bbr2_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        throughput_cubic, throughput_bbr2, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for i,d in zip(range(len(data)),data):\n",
    "            try:\n",
    "                if \"interval\" in d.keys() and i==0:\n",
    "                    start_time = d['interval']['time'] # Start time of the test\n",
    "                if \"streams\" in d.keys():\n",
    "                    end_time = data[i-1]['interval']['time'] # End time of the test\n",
    "                    mss = 9000 # data[i-1]['interval']['mss'] # maximum segment size\n",
    "                    \n",
    "                    cubic_data_seg_list, bbr2_data_seg_list = [], []\n",
    "                    for j in range(len(d['streams'])):\n",
    "                        if \"cubic\" in d['streams'][j]['cc']:\n",
    "                            cubic_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                        elif \"bbr2\" in d['streams'][j]['cc']:\n",
    "                            bbr2_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                    \n",
    "                    data_seg_sum_cubic.append( sum(cubic_data_seg_list) )\n",
    "                    throughput_cubic = (sum(cubic_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_cubic_p16.append( throughput_cubic )\n",
    "\n",
    "                    data_seg_sum_bbr2.append( sum(bbr2_data_seg_list) )\n",
    "                    throughput_bbr2 = (sum(bbr2_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_bbr2_p16.append( throughput_bbr2 )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"Cubic Mean: {np.mean(tput_cubic_p16)}  | Cubic Variance: {np.var(tput_cubic_p16)}\")\n",
    "print(f\"Bbrv2 Mean: {np.mean(tput_bbr2_p16)}  | Bbrv2 Variance: {np.var(tput_bbr2_p16)}\")\n",
    "\n",
    "print(\"Data Segment\")\n",
    "print(f\"Cubic Mean: {np.mean(data_seg_sum_cubic)}  | Cubic Variance: {np.var(data_seg_sum_cubic)}\")\n",
    "print(f\"Bbrv2 Mean: {np.mean(data_seg_sum_bbr2)}  | Bbrv2 Variance: {np.var(data_seg_sum_bbr2)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 50\n",
      "Throughput\n",
      "Cubic Mean: 3.8228811904506266  | Cubic Variance: 7.690460406763658\n",
      "Bbrv2 Mean: 14.199113723016668  | Bbrv2 Variance: 210.9804999420399\n",
      "Data Segment\n",
      "Cubic Mean: 8785820.88  | Cubic Variance: 218978255872263.25\n",
      "Bbrv2 Mean: 34885604.12  | Bbrv2 Variance: 3816636072264946.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "==========================================\n",
    "## bost-dtn (2021-06-16:02:12)\n",
    "### 1. bbr2_p1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "pscheduler_bbr2_p1  = os.path.join(rootdir, \"2021-06-16:02:12/pscheduler_bbr2_p1\")\n",
    "filenames = traverse(pscheduler_bbr2_p1)\n",
    "\n",
    "data_seg = []\n",
    "tput_bbr2_p1 = []\n",
    "key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and i==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key2 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                bbr2_data_seg = data[j][key2]\n",
    "                data_seg.append( bbr2_data_seg )\n",
    "\n",
    "                throughput = (bbr2_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                tput_bbr2_p1.append( throughput )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"Mean: {np.mean(tput_bbr2_p1)}  | Variance: {np.var(tput_bbr2_p1)}\")\n",
    "print(\"Data Segment\")\n",
    "print(f\"Mean: {np.mean(data_seg)}  | Variance: {np.var(data_seg)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 55\n",
      "float division by zero\n",
      "Throughput\n",
      "Mean: 2.3658483357760926e-07  | Variance: 1.1252410355603162e-14\n",
      "Data Segment\n",
      "Mean: 11637878.672727272  | Variance: 59359636395545.72\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bost-dtn (2021-06-16:02:12)\n",
    "### 2. cubic_p1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "pscheduler_cubic_p1 = os.path.join(rootdir, \"2021-06-16:02:12/pscheduler_cubic_p1\")\n",
    "filenames = traverse(pscheduler_cubic_p1)\n",
    "\n",
    "data_seg = []\n",
    "tput_p1_cubic = []\n",
    "key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        cubic_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and i==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key1 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                cubic_data_seg = data[j][key1]\n",
    "                data_seg.append( cubic_data_seg )\n",
    "\n",
    "                throughput = (cubic_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                tput_p1_cubic.append( throughput )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"Mean: {np.mean(tput_p1_cubic)}  | Variance: {np.var(tput_p1_cubic)}\")\n",
    "print(\"Data Segment\")\n",
    "print(f\"Mean: {np.mean(data_seg)}  | Variance: {np.var(data_seg)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 54\n",
      "float division by zero\n",
      "Throughput\n",
      "Mean: 2.372379213725238e-07  | Variance: 1.319571165204607e-14\n",
      "Data Segment\n",
      "Mean: 11014398.944444444  | Variance: 62556039019839.65\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bost-dtn (2021-06-16:02:12)\n",
    "### 3. both_p16"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "pscheduler_both_p16 = os.path.join(rootdir, \"2021-06-16:02:12/pscheduler_both_p16\")\n",
    "filenames = traverse(pscheduler_both_p16)\n",
    "\n",
    "data_seg_sum_cubic, data_seg_sum_bbr2 = [], []\n",
    "tput_cubic_p16, tput_bbr2_p16 = [], []\n",
    "key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        throughput_cubic, throughput_bbr2, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for i,d in zip(range(len(data)),data):\n",
    "            try:\n",
    "                if \"interval\" in d.keys() and i==0:\n",
    "                    start_time = d['interval']['time'] # Start time of the test\n",
    "                if \"streams\" in d.keys():\n",
    "                    end_time = data[i-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[i-1]['interval']['mss'] # maximum segment size\n",
    "                    \n",
    "                    cubic_data_seg_list, bbr2_data_seg_list = [], []\n",
    "                    for j in range(len(d['streams'])):\n",
    "                        if \"cubic\" in d['streams'][j]['cc']:\n",
    "                            cubic_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                        elif \"bbr2\" in d['streams'][j]['cc']:\n",
    "                            bbr2_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                    \n",
    "                    data_seg_sum_cubic.append( sum(cubic_data_seg_list) )\n",
    "                    throughput_cubic = (sum(cubic_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_cubic_p16.append( throughput_cubic )\n",
    "\n",
    "                    data_seg_sum_bbr2.append( sum(bbr2_data_seg_list) )\n",
    "                    throughput_bbr2 = (sum(bbr2_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_bbr2_p16.append( throughput_bbr2 )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"Cubic Mean: {np.mean(tput_cubic_p16)}  | Cubic Variance: {np.var(tput_cubic_p16)}\")\n",
    "print(f\"Bbrv2 Mean: {np.mean(tput_bbr2_p16)}  | Bbrv2 Variance: {np.var(tput_bbr2_p16)}\")\n",
    "\n",
    "print(\"Data Segment\")\n",
    "print(f\"Cubic Mean: {np.mean(data_seg_sum_cubic)}  | Cubic Variance: {np.var(data_seg_sum_cubic)}\")\n",
    "print(f\"Bbrv2 Mean: {np.mean(data_seg_sum_bbr2)}  | Bbrv2 Variance: {np.var(data_seg_sum_bbr2)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 51\n",
      "Throughput\n",
      "Cubic Mean: 1.7949846054399416  | Cubic Variance: 3.1648783837292944\n",
      "Bbrv2 Mean: 6.21102808596121  | Bbrv2 Variance: 3.99020888241716\n",
      "Data Segment\n",
      "Cubic Mean: 7093619.980392157  | Cubic Variance: 174546858285969.25\n",
      "Bbrv2 Mean: 34174089.37254902  | Bbrv2 Variance: 2689454270182730.5\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Buffer Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e443ef68753a57e21b26925269da7f3b0b849de1f4d00950fc0d90accc0b6012"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}