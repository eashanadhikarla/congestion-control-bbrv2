{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Bost-dtn Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "rootdir = \"/Users/eashan22/Desktop/Internship 2021/bbrv2/Brian's Project/bost-dtn\"\n",
    "# rootdir = \"/home/eadhikarla/Desktop/Brian's Project/bost-dtn\"\n",
    "\n",
    "def traverse(path):\n",
    "    fileList = []\n",
    "    c = 0\n",
    "    for root, directories, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\"json\"):\n",
    "                fileList.append(os.path.join(root,file))\n",
    "                c+=1\n",
    "    print(f\"Total files: {c}\")\n",
    "    return fileList"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "==========================================\n",
    "## bost-dtn (2021-06-09:14:35)\n",
    "### 1. bbr2_p1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "pscheduler_bbr2_p1  = os.path.join(rootdir, \"2021-06-09:14:35/pscheduler_bbr2_p1\")\n",
    "filenames = traverse(pscheduler_bbr2_p1)\n",
    "\n",
    "data_seg = []\n",
    "tput_bbr2_p1 = []\n",
    "key2 = 'bbr2_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            \n",
    "            if \"interval\" in d.keys() and j==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key2 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                bbr2_data_seg = data[j][key2]\n",
    "                data_seg.append(bbr2_data_seg)\n",
    "\n",
    "                throughput = ((bbr2_data_seg*mss*8)/(end_time-start_time))/1e9\n",
    "                tput_bbr2_p1.append( throughput )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "print(\"Throughput\")\n",
    "print(f\"Mean: {np.mean(tput_bbr2_p1):.5f}  | Variance: {np.var(tput_bbr2_p1):.5f}\")\n",
    "print(\"Data Segment\")\n",
    "print(f\"Mean: {np.mean(data_seg):.5f}  | Variance: {np.var(data_seg):.5f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 105\n",
      "Throughput\n",
      "Mean: 6.01623  | Variance: 6.99823\n",
      "Data Segment\n",
      "Mean: 10593937.79048  | Variance: 53354882026455.64844\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bost-dtn (2021-06-09:14:35)\n",
    "### 2. cubic_p1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "pscheduler_cubic_p1 = os.path.join(rootdir, \"2021-06-09:14:35/pscheduler_cubic_p1\")\n",
    "filenames = traverse(pscheduler_cubic_p1)\n",
    "\n",
    "data_seg = []\n",
    "tput_p1_cubic = []\n",
    "key1 = 'cubic_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        cubic_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and j==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key1 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                cubic_data_seg = data[j][key1]\n",
    "                data_seg.append(cubic_data_seg)\n",
    "\n",
    "                throughput = ((cubic_data_seg*mss*8)/(end_time-start_time))/1e9\n",
    "                tput_p1_cubic.append( throughput )\n",
    "        # print(f\"Throughput: {(throughput/1e9):.4f} Gbps/sec\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"Mean: {np.mean(tput_p1_cubic):.5f}  | Variance: {np.var(tput_p1_cubic):.5f}\")\n",
    "print(\"Data Segment\")\n",
    "print(f\"Mean: {np.mean(data_seg):.5f}  | Variance: {np.var(data_seg):.5f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 105\n",
      "Throughput\n",
      "Mean: 6.52317  | Variance: 8.67180\n",
      "Data Segment\n",
      "Mean: 11281322.24762  | Variance: 59836109647663.10156\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bost-dtn (2021-06-09:14:35)\n",
    "### 3. both_p16  -----> Fig. 4b. in paper"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "pscheduler_both_p16 = os.path.join(rootdir, \"2021-06-09:14:35/pscheduler_both_p16\")\n",
    "filenames = traverse(pscheduler_both_p16)\n",
    "\n",
    "data_seg_sum_cubic, data_seg_sum_bbr2 = [], []\n",
    "tput_cubic_p16, tput_bbr2_p16 = [], []\n",
    "key1, key2 = 'cubic_data_segs', 'bbr2_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        throughput_cubic, throughput_bbr2, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in zip(range(len(data)),data):\n",
    "            try:\n",
    "                if \"interval\" in d.keys() and j==0:\n",
    "                    start_time = d['interval']['time'] # Start time of the test\n",
    "                if \"streams\" in d.keys():\n",
    "                    end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "                    \n",
    "                    cubic_data_seg_list, bbr2_data_seg_list = [], []\n",
    "                    for j in range(len(d['streams'])):\n",
    "                        if \"cubic\" in d['streams'][j]['cc']:\n",
    "                            cubic_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                        elif \"bbr2\" in d['streams'][j]['cc']:\n",
    "                            bbr2_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                    \n",
    "                    data_seg_sum_cubic.append( sum(cubic_data_seg_list) )\n",
    "                    throughput_cubic = ((sum(cubic_data_seg_list)*mss*8)/(end_time-start_time))/1e9\n",
    "                    tput_cubic_p16.append( throughput_cubic )\n",
    "\n",
    "                    data_seg_sum_bbr2.append( sum(bbr2_data_seg_list) )\n",
    "                    throughput_bbr2 = ((sum(bbr2_data_seg_list)*mss*8)/(end_time-start_time))/1e9\n",
    "                    tput_bbr2_p16.append( throughput_bbr2 )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"Cubic Mean: {np.mean(tput_cubic_p16):.5f}  | Cubic Variance: {np.var(tput_cubic_p16):.5f}\")\n",
    "print(f\"Bbrv2 Mean: {np.mean(tput_bbr2_p16):.5f}  | Bbrv2 Variance: {np.var(tput_bbr2_p16):.5f}\")\n",
    "\n",
    "print(\"Data Segment\")\n",
    "print(f\"Cubic Mean: {np.mean(data_seg_sum_cubic):.5f}  | Cubic Variance: {np.var(data_seg_sum_cubic):.5f}\")\n",
    "print(f\"Bbrv2 Mean: {np.mean(data_seg_sum_bbr2):.5f}  | Bbrv2 Variance: {np.var(data_seg_sum_bbr2):.5f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 109\n",
      "Throughput\n",
      "Cubic Mean: 4.30749  | Cubic Variance: 4.02542\n",
      "Bbrv2 Mean: 3.42120  | Bbrv2 Variance: 1.68668\n",
      "Data Segment\n",
      "Cubic Mean: 19844388.68807  | Cubic Variance: 1702568119688929.50000\n",
      "Bbrv2 Mean: 16160038.23853  | Bbrv2 Variance: 663028364607013.12500\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "==========================================\n",
    "## bost-dtn (2021-06-15:02:12)\n",
    "### 1. bbr2_p1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "pscheduler_bbr2_p1  = os.path.join(rootdir, \"2021-06-15:02:12/pscheduler_bbr2_p1\")\n",
    "filenames = traverse(pscheduler_bbr2_p1)\n",
    "\n",
    "data_seg = []\n",
    "tput_bbr2_p1 = []\n",
    "key2 = 'bbr2_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and j==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key2 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                bbr2_data_seg = data[j][key2]\n",
    "                data_seg.append(bbr2_data_seg)\n",
    "\n",
    "                throughput = ((bbr2_data_seg*mss*8)/(end_time-start_time))/1e9\n",
    "                tput_bbr2_p1.append( throughput )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"Mean: {np.mean(tput_bbr2_p1):.5f}  | Variance: {np.var(tput_bbr2_p1):.5f}\")\n",
    "print(\"Data Segment\")\n",
    "print(f\"Mean: {np.mean(data_seg):.5f}  | Variance: {np.var(data_seg):.5f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 45\n",
      "Throughput\n",
      "Mean: 6.73127  | Variance: 8.10835\n",
      "Data Segment\n",
      "Mean: 9662307.77778  | Variance: 46214551992636.75781\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bost-dtn (2021-06-15:02:12)\n",
    "### 2. cubic_p1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "pscheduler_cubic_p1 = os.path.join(rootdir, \"2021-06-15:02:12/pscheduler_cubic_p1\")\n",
    "filenames = traverse(pscheduler_cubic_p1)\n",
    "\n",
    "data_seg = []\n",
    "tput_p1_cubic = []\n",
    "key1 = 'cubic_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        cubic_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and j==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key1 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                cubic_data_seg = data[j][key1]\n",
    "                data_seg.append(cubic_data_seg)\n",
    "\n",
    "                throughput = ((cubic_data_seg*mss*8)/(end_time-start_time))/1e9\n",
    "                tput_p1_cubic.append( throughput )\n",
    "        # print(f\"Throughput: {(throughput/1e9):.4f} Gbps/sec\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"Mean: {np.mean(tput_p1_cubic):.5f}  | Variance: {np.var(tput_p1_cubic):.5f}\")\n",
    "print(\"Data Segment\")\n",
    "print(f\"Mean: {np.mean(data_seg):.5f}  | Variance: {np.var(data_seg):.5f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 47\n",
      "Throughput\n",
      "Mean: 6.71882  | Variance: 9.18101\n",
      "Data Segment\n",
      "Mean: 9722667.61702  | Variance: 47272544227031.08594\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bost-dtn (2021-06-15:02:12)\n",
    "### 3. both_p16"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "pscheduler_both_p16 = os.path.join(rootdir, \"2021-06-15:02:12/pscheduler_both_p16\")\n",
    "filenames = traverse(pscheduler_both_p16)\n",
    "\n",
    "data_seg_sum_cubic, data_seg_sum_bbr2 = [], []\n",
    "tput_cubic_p16, tput_bbr2_p16 = [], []\n",
    "key1, key2 = 'cubic_data_segs', 'bbr2_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        throughput_cubic, throughput_bbr2, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in zip(range(len(data)),data):\n",
    "            try:\n",
    "                if \"interval\" in d.keys() and j==0:\n",
    "                    start_time = d['interval']['time'] # Start time of the test\n",
    "                if \"streams\" in d.keys():\n",
    "                    end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "                    \n",
    "                    cubic_data_seg_list, bbr2_data_seg_list = [], []\n",
    "                    for j in range(len(d['streams'])):\n",
    "                        if \"cubic\" in d['streams'][j]['cc']:\n",
    "                            cubic_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                        elif \"bbr2\" in d['streams'][j]['cc']:\n",
    "                            bbr2_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                    \n",
    "                    data_seg_sum_cubic.append( sum(cubic_data_seg_list) )\n",
    "                    throughput_cubic = ((sum(cubic_data_seg_list)*mss*8)/(end_time-start_time))/1e9\n",
    "                    tput_cubic_p16.append( throughput_cubic )\n",
    "\n",
    "                    data_seg_sum_bbr2.append( sum(bbr2_data_seg_list) )\n",
    "                    throughput_bbr2 = ((sum(bbr2_data_seg_list)*mss*8)/(end_time-start_time))/1e9\n",
    "                    tput_bbr2_p16.append( throughput_bbr2 )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"Cubic Mean: {np.mean(tput_cubic_p16):.5f}  | Cubic Variance: {np.var(tput_cubic_p16):.5f}\")\n",
    "print(f\"Bbrv2 Mean: {np.mean(tput_bbr2_p16):.5f}  | Bbrv2 Variance: {np.var(tput_bbr2_p16):.5f}\")\n",
    "\n",
    "print(\"Data Segment\")\n",
    "print(f\"Cubic Mean: {np.mean(data_seg_sum_cubic):.5f}  | Cubic Variance: {np.var(data_seg_sum_cubic):.5f}\")\n",
    "print(f\"Bbrv2 Mean: {np.mean(data_seg_sum_bbr2):.5f}  | Bbrv2 Variance: {np.var(data_seg_sum_bbr2):.5f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 50\n",
      "Throughput\n",
      "Cubic Mean: 2.36102  | Cubic Variance: 4.05161\n",
      "Bbrv2 Mean: 5.80051  | Bbrv2 Variance: 4.02567\n",
      "Data Segment\n",
      "Cubic Mean: 8785820.88000  | Cubic Variance: 218978255872263.18750\n",
      "Bbrv2 Mean: 34885604.12000  | Bbrv2 Variance: 3816636072264945.50000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "==========================================\n",
    "## bost-dtn (2021-06-16:02:12)\n",
    "### 1. bbr2_p1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "pscheduler_bbr2_p1  = os.path.join(rootdir, \"2021-06-16:02:12/pscheduler_bbr2_p1\")\n",
    "filenames = traverse(pscheduler_bbr2_p1)\n",
    "\n",
    "data_seg = []\n",
    "tput_bbr2_p1 = []\n",
    "key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and j==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key2 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                bbr2_data_seg = data[j][key2]\n",
    "                data_seg.append( bbr2_data_seg )\n",
    "\n",
    "                throughput = (bbr2_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                tput_bbr2_p1.append( throughput )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"Mean: {np.mean(tput_bbr2_p1):.5f}  | Variance: {np.var(tput_bbr2_p1):.5f}\")\n",
    "print(\"Data Segment\")\n",
    "print(f\"Mean: {np.mean(data_seg):.5f}  | Variance: {np.var(data_seg):.5f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 55\n",
      "Throughput\n",
      "Mean: 6.33176  | Variance: 8.16012\n",
      "Data Segment\n",
      "Mean: 11637878.67273  | Variance: 59359636395545.71094\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bost-dtn (2021-06-16:02:12)\n",
    "### 2. cubic_p1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "pscheduler_cubic_p1 = os.path.join(rootdir, \"2021-06-16:02:12/pscheduler_cubic_p1\")\n",
    "filenames = traverse(pscheduler_cubic_p1)\n",
    "\n",
    "data_seg = []\n",
    "tput_p1_cubic = []\n",
    "key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        cubic_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and j==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key1 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                cubic_data_seg = data[j][key1]\n",
    "                data_seg.append( cubic_data_seg )\n",
    "\n",
    "                throughput = (cubic_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                tput_p1_cubic.append( throughput )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"Mean: {np.mean(tput_p1_cubic):.5f}  | Variance: {np.var(tput_p1_cubic):.5f}\")\n",
    "print(\"Data Segment\")\n",
    "print(f\"Mean: {np.mean(data_seg):.5f}  | Variance: {np.var(data_seg):.5f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 54\n",
      "Throughput\n",
      "Mean: 6.34994  | Variance: 9.53285\n",
      "Data Segment\n",
      "Mean: 11014398.94444  | Variance: 62556039019839.64844\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bost-dtn (2021-06-16:02:12)\n",
    "### 3. both_p16"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "pscheduler_both_p16 = os.path.join(rootdir, \"2021-06-16:02:12/pscheduler_both_p16\")\n",
    "filenames = traverse(pscheduler_both_p16)\n",
    "\n",
    "data_seg_sum_cubic, data_seg_sum_bbr2 = [], []\n",
    "tput_cubic_p16, tput_bbr2_p16 = [], []\n",
    "key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        throughput_cubic, throughput_bbr2, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in zip(range(len(data)),data):\n",
    "            try:\n",
    "                if \"interval\" in d.keys() and j==0:\n",
    "                    start_time = d['interval']['time'] # Start time of the test\n",
    "                if \"streams\" in d.keys():\n",
    "                    end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "                    \n",
    "                    cubic_data_seg_list, bbr2_data_seg_list = [], []\n",
    "                    for j in range(len(d['streams'])):\n",
    "                        if \"cubic\" in d['streams'][j]['cc']:\n",
    "                            cubic_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                        elif \"bbr2\" in d['streams'][j]['cc']:\n",
    "                            bbr2_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                    \n",
    "                    data_seg_sum_cubic.append( sum(cubic_data_seg_list) )\n",
    "                    throughput_cubic = (sum(cubic_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_cubic_p16.append( throughput_cubic )\n",
    "\n",
    "                    data_seg_sum_bbr2.append( sum(bbr2_data_seg_list) )\n",
    "                    throughput_bbr2 = (sum(bbr2_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_bbr2_p16.append( throughput_bbr2 )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"Cubic Mean: {np.mean(tput_cubic_p16):.5f}  | Cubic Variance: {np.var(tput_cubic_p16):.5f}\")\n",
    "print(f\"Bbrv2 Mean: {np.mean(tput_bbr2_p16):.5f}  | Bbrv2 Variance: {np.var(tput_bbr2_p16):.5f}\")\n",
    "\n",
    "print(\"Data Segment\")\n",
    "print(f\"Cubic Mean: {np.mean(data_seg_sum_cubic):.5f}  | Cubic Variance: {np.var(data_seg_sum_cubic):.5f}\")\n",
    "print(f\"Bbrv2 Mean: {np.mean(data_seg_sum_bbr2):.5f}  | Bbrv2 Variance: {np.var(data_seg_sum_bbr2):.5f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 51\n",
      "Throughput\n",
      "Cubic Mean: 1.79498  | Cubic Variance: 3.16488\n",
      "Bbrv2 Mean: 6.21103  | Bbrv2 Variance: 3.99021\n",
      "Data Segment\n",
      "Cubic Mean: 7093619.98039  | Cubic Variance: 174546858285969.28125\n",
      "Bbrv2 Mean: 34174089.37255  | Bbrv2 Variance: 2689454270182731.00000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e443ef68753a57e21b26925269da7f3b0b849de1f4d00950fc0d90accc0b6012"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}