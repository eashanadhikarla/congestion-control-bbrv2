{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Bost-dtn Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "rootdir = \"/Users/eashan22/Desktop/Internship 2021/bbrv2/Brian's Project/bost-dtn\"\n",
    "# rootdir = \"/home/eadhikarla/Desktop/Brian's Project/bost-dtn\"\n",
    "\n",
    "def traverse(path):\n",
    "    fileList = []\n",
    "    c = 0\n",
    "    for root, directories, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\"json\"):\n",
    "                fileList.append(os.path.join(root,file))\n",
    "                c+=1\n",
    "    print(f\"Total files: {c}\")\n",
    "    return fileList"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "==========================================\n",
    "## bost-dtn (2021-06-09:14:35)\n",
    "### 1. bbr2_p1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "pscheduler_bbr2_p1  = os.path.join(rootdir, \"2021-06-09:14:35/pscheduler_bbr2_p1\")\n",
    "filenames = traverse(pscheduler_bbr2_p1)\n",
    "\n",
    "data_seg = []\n",
    "tput_bbr2_p1 = []\n",
    "key2 = 'bbr2_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            \n",
    "            if \"interval\" in d.keys() and j==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key2 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                bbr2_data_seg = data[j][key2]\n",
    "                data_seg.append(bbr2_data_seg)\n",
    "\n",
    "                throughput = ((bbr2_data_seg*mss*8)/(end_time-start_time))/1e9\n",
    "                tput_bbr2_p1.append( throughput )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "print(\"Throughput\")\n",
    "print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p1):.5f}  |  Std. Dev.: {np.std(tput_bbr2_p1):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p1)/np.mean(tput_bbr2_p1)):.5f}  |  Variance: {np.var(tput_bbr2_p1):.5f}\")\n",
    "print(\"Data Segment\")\n",
    "print(f\"BBRv2 - Mean: {np.mean(data_seg):.5f}  |  Std. Dev.: {np.std(data_seg):.5f}  |  Coef. of Variance: {(np.std(data_seg)/np.mean(data_seg)):.5f}  |  Variance: {np.var(data_seg):.5f}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 105\n",
      "Throughput\n",
      "BBRv2 - Mean: 6.01623  |  Std. Dev.: 2.64542  |  Coef. of Variance: 0.43971  |  Variance: 6.99823\n",
      "Data Segment\n",
      "BBRv2 - Mean: 10593937.79048  |  Std. Dev.: 7304442.62257  |  Coef. of Variance: 0.68949  |  Variance: 53354882026455.64844\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bost-dtn (2021-06-09:14:35)\n",
    "### 2. cubic_p1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "pscheduler_cubic_p1 = os.path.join(rootdir, \"2021-06-09:14:35/pscheduler_cubic_p1\")\n",
    "filenames = traverse(pscheduler_cubic_p1)\n",
    "\n",
    "data_seg = []\n",
    "tput_p1_cubic = []\n",
    "key1 = 'cubic_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        cubic_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and j==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key1 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                cubic_data_seg = data[j][key1]\n",
    "                data_seg.append(cubic_data_seg)\n",
    "\n",
    "                throughput = ((cubic_data_seg*mss*8)/(end_time-start_time))/1e9\n",
    "                tput_p1_cubic.append( throughput )\n",
    "        # print(f\"Throughput: {(throughput/1e9):.4f} Gbps/sec\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"CUBIC - Mean: {np.mean(tput_p1_cubic):.5f}  |  Std. Dev.: {np.std(tput_p1_cubic):.5f}  |  Coef. of Variance: {(np.std(tput_p1_cubic)/np.mean(tput_p1_cubic)):.5f}  |  Variance: {np.var(tput_p1_cubic):.5f}\")\n",
    "print(\"Data Segment\")\n",
    "print(f\"CUBIC - Mean: {np.mean(data_seg):.5f}  |  Std. Dev.: {np.std(data_seg):.5f}  |  Coef. of Variance: {(np.std(data_seg)/np.mean(data_seg)):.5f}  |  Variance: {np.var(data_seg):.5f}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 105\n",
      "Throughput\n",
      "CUBIC - Mean: 6.52317  |  Std. Dev.: 2.94479  |  Coef. of Variance: 0.45144  |  Variance: 8.67180\n",
      "Data Segment\n",
      "CUBIC - Mean: 11281322.24762  |  Std. Dev.: 7735380.38158  |  Coef. of Variance: 0.68568  |  Variance: 59836109647663.10156\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bost-dtn (2021-06-09:14:35)\n",
    "### 3. both_p16  -----> Fig. 4b. in paper"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "pscheduler_both_p16 = os.path.join(rootdir, \"2021-06-09:14:35/pscheduler_both_p16\")\n",
    "filenames = traverse(pscheduler_both_p16)\n",
    "\n",
    "data_seg_sum_cubic, data_seg_sum_bbr2 = [], []\n",
    "tput_cubic_p16, tput_bbr2_p16 = [], []\n",
    "key1, key2 = 'cubic_data_segs', 'bbr2_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        throughput_cubic, throughput_bbr2, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in zip(range(len(data)),data):\n",
    "            try:\n",
    "                if \"interval\" in d.keys() and j==0:\n",
    "                    start_time = d['interval']['time'] # Start time of the test\n",
    "                if \"streams\" in d.keys():\n",
    "                    end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "                    \n",
    "                    cubic_data_seg_list, bbr2_data_seg_list = [], []\n",
    "                    for j in range(len(d['streams'])):\n",
    "                        if \"cubic\" in d['streams'][j]['cc']:\n",
    "                            cubic_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                        elif \"bbr2\" in d['streams'][j]['cc']:\n",
    "                            bbr2_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                    \n",
    "                    data_seg_sum_cubic.append( sum(cubic_data_seg_list) )\n",
    "                    throughput_cubic = ((sum(cubic_data_seg_list)*mss*8)/(end_time-start_time))/1e9\n",
    "                    tput_cubic_p16.append( throughput_cubic )\n",
    "\n",
    "                    data_seg_sum_bbr2.append( sum(bbr2_data_seg_list) )\n",
    "                    throughput_bbr2 = ((sum(bbr2_data_seg_list)*mss*8)/(end_time-start_time))/1e9\n",
    "                    tput_bbr2_p16.append( throughput_bbr2 )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"CUBIC - Mean: {np.mean(tput_cubic_p16):.5f}  |  Std. Dev.: {np.std(tput_cubic_p16):.5f}  |  Coef. of Variance: {(np.std(tput_cubic_p16)/np.mean(tput_cubic_p16)):.5f}  |  Variance: {np.var(tput_cubic_p16):.5f}\")\n",
    "print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p16):.5f}  |  Std. Dev.: {np.std(tput_bbr2_p16):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p16)/np.mean(tput_bbr2_p16)):.5f}  |  Variance: {np.var(tput_bbr2_p16):.5f}\")\n",
    "\n",
    "print(\"Data Segment\")\n",
    "print(f\"CUBIC - Mean: {np.mean(data_seg_sum_cubic):.5f}  |  Std. Dev.: {np.std(data_seg_sum_cubic):.5f}  |  Coef. of Variance: {(np.std(data_seg_sum_cubic)/np.mean(data_seg_sum_cubic)):.5f}  |  Variance: {np.var(data_seg_sum_cubic):.5f}\")\n",
    "print(f\"BBRv2 - Mean: {np.mean(data_seg_sum_bbr2):.5f}  |  Std. Dev.: {np.std(data_seg_sum_bbr2):.5f}  |  Coef. of Variance: {(np.std(data_seg_sum_bbr2)/np.mean(data_seg_sum_bbr2)):.5f}  |  Variance: {np.var(data_seg_sum_bbr2):.5f}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 109\n",
      "Throughput\n",
      "CUBIC - Mean: 4.30749  |  Std. Dev.: 2.00634  |  Coef. of Variance: 0.46578  |  Variance: 4.02542\n",
      "BBRv2 - Mean: 3.42120  |  Std. Dev.: 1.29872  |  Coef. of Variance: 0.37961  |  Variance: 1.68668\n",
      "Data Segment\n",
      "CUBIC - Mean: 19844388.68807  |  Std. Dev.: 41262187.52913  |  Coef. of Variance: 2.07929  |  Variance: 1702568119688929.50000\n",
      "BBRv2 - Mean: 16160038.23853  |  Std. Dev.: 25749337.16830  |  Coef. of Variance: 1.59340  |  Variance: 663028364607013.12500\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "==========================================\n",
    "## bost-dtn (2021-06-15:02:12)\n",
    "### 1. bbr2_p1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "pscheduler_bbr2_p1  = os.path.join(rootdir, \"2021-06-15:02:12/pscheduler_bbr2_p1\")\n",
    "filenames = traverse(pscheduler_bbr2_p1)\n",
    "\n",
    "data_seg = []\n",
    "tput_bbr2_p1 = []\n",
    "key2 = 'bbr2_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and j==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key2 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                bbr2_data_seg = data[j][key2]\n",
    "                data_seg.append(bbr2_data_seg)\n",
    "\n",
    "                throughput = ((bbr2_data_seg*mss*8)/(end_time-start_time))/1e9\n",
    "                tput_bbr2_p1.append( throughput )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p1):.5f}  |  Std. Dev.: {np.std(tput_bbr2_p1):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p1)/np.mean(tput_bbr2_p1)):.5f}  |  Variance: {np.var(tput_bbr2_p1):.5f}\")\n",
    "print(\"Data Segment\")\n",
    "print(f\"BBRv2 - Mean: {np.mean(data_seg):.5f}  |  Std. Dev.: {np.std(data_seg):.5f}  |  Coef. of Variance: {(np.std(data_seg)/np.mean(data_seg)):.5f}  |  Variance: {np.var(data_seg):.5f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 45\n",
      "Throughput\n",
      "BBRv2 - Mean: 6.73127  |  Std. Dev.: 2.84752  |  Coef. of Variance: 0.42303  |  Variance: 8.10835\n",
      "Data Segment\n",
      "BBRv2 - Mean: 9662307.77778  |  Std. Dev.: 6798128.56547  |  Coef. of Variance: 0.70357  |  Variance: 46214551992636.75781\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bost-dtn (2021-06-15:02:12)\n",
    "### 2. cubic_p1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "pscheduler_cubic_p1 = os.path.join(rootdir, \"2021-06-15:02:12/pscheduler_cubic_p1\")\n",
    "filenames = traverse(pscheduler_cubic_p1)\n",
    "\n",
    "data_seg = []\n",
    "tput_p1_cubic = []\n",
    "key1 = 'cubic_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        cubic_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and j==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key1 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                cubic_data_seg = data[j][key1]\n",
    "                data_seg.append(cubic_data_seg)\n",
    "\n",
    "                throughput = ((cubic_data_seg*mss*8)/(end_time-start_time))/1e9\n",
    "                tput_p1_cubic.append( throughput )\n",
    "        # print(f\"Throughput: {(throughput/1e9):.4f} Gbps/sec\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"CUBIC - Mean: {np.mean(tput_p1_cubic):.5f}  |  Std. Dev.: {np.std(tput_p1_cubic):.5f}  |  Coef. of Variance: {(np.std(tput_p1_cubic)/np.mean(tput_p1_cubic)):.5f}  |  Variance: {np.var(tput_p1_cubic):.5f}\")\n",
    "print(\"Data Segment\")\n",
    "print(f\"CUBIC - Mean: {np.mean(data_seg):.5f}  |  Std. Dev.: {np.std(data_seg):.5f}  |  Coef. of Variance: {(np.std(data_seg)/np.mean(data_seg)):.5f}  |  Variance: {np.var(data_seg):.5f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 47\n",
      "Throughput\n",
      "CUBIC - Mean: 6.71882  |  Std. Dev.: 3.03002  |  Coef. of Variance: 0.45097  |  Variance: 9.18101\n",
      "Data Segment\n",
      "CUBIC - Mean: 9722667.61702  |  Std. Dev.: 6875503.19810  |  Coef. of Variance: 0.70716  |  Variance: 47272544227031.08594\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bost-dtn (2021-06-15:02:12)\n",
    "### 3. both_p16"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "pscheduler_both_p16 = os.path.join(rootdir, \"2021-06-15:02:12/pscheduler_both_p16\")\n",
    "filenames = traverse(pscheduler_both_p16)\n",
    "\n",
    "data_seg_sum_cubic, data_seg_sum_bbr2 = [], []\n",
    "tput_cubic_p16, tput_bbr2_p16 = [], []\n",
    "key1, key2 = 'cubic_data_segs', 'bbr2_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        throughput_cubic, throughput_bbr2, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in zip(range(len(data)),data):\n",
    "            try:\n",
    "                if \"interval\" in d.keys() and j==0:\n",
    "                    start_time = d['interval']['time'] # Start time of the test\n",
    "                if \"streams\" in d.keys():\n",
    "                    end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "                    \n",
    "                    cubic_data_seg_list, bbr2_data_seg_list = [], []\n",
    "                    for j in range(len(d['streams'])):\n",
    "                        if \"cubic\" in d['streams'][j]['cc']:\n",
    "                            cubic_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                        elif \"bbr2\" in d['streams'][j]['cc']:\n",
    "                            bbr2_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                    \n",
    "                    data_seg_sum_cubic.append( sum(cubic_data_seg_list) )\n",
    "                    throughput_cubic = ((sum(cubic_data_seg_list)*mss*8)/(end_time-start_time))/1e9\n",
    "                    tput_cubic_p16.append( throughput_cubic )\n",
    "\n",
    "                    data_seg_sum_bbr2.append( sum(bbr2_data_seg_list) )\n",
    "                    throughput_bbr2 = ((sum(bbr2_data_seg_list)*mss*8)/(end_time-start_time))/1e9\n",
    "                    tput_bbr2_p16.append( throughput_bbr2 )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"CUBIC - Mean: {np.mean(tput_cubic_p16):.5f}  |  Std. Dev.: {np.std(tput_cubic_p16):.5f}  |  Coef. of Variance: {(np.std(tput_cubic_p16)/np.mean(tput_cubic_p16)):.5f}  |  Variance: {np.var(tput_cubic_p16):.5f}\")\n",
    "print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p16):.5f}  |  Std. Dev.: {np.std(tput_bbr2_p16):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p16)/np.mean(tput_bbr2_p16)):.5f}  |  Variance: {np.var(tput_bbr2_p16):.5f}\")\n",
    "\n",
    "print(\"Data Segment\")\n",
    "print(f\"CUBIC - Mean: {np.mean(data_seg_sum_cubic):.5f}  |  Std. Dev.: {np.std(data_seg_sum_cubic):.5f}  |  Coef. of Variance: {(np.std(data_seg_sum_cubic)/np.mean(data_seg_sum_cubic)):.5f}  |  Variance: {np.var(data_seg_sum_cubic):.5f}\")\n",
    "print(f\"BBRv2 - Mean: {np.mean(data_seg_sum_bbr2):.5f}  |  Std. Dev.: {np.std(data_seg_sum_bbr2):.5f}  |  Coef. of Variance: {(np.std(data_seg_sum_bbr2)/np.mean(data_seg_sum_bbr2)):.5f}  |  Variance: {np.var(data_seg_sum_bbr2):.5f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 50\n",
      "Throughput\n",
      "CUBIC - Mean: 2.36102  |  Std. Dev.: 2.01286  |  Coef. of Variance: 0.85254  |  Variance: 4.05161\n",
      "BBRv2 - Mean: 5.80051  |  Std. Dev.: 2.00641  |  Coef. of Variance: 0.34590  |  Variance: 4.02567\n",
      "Data Segment\n",
      "CUBIC - Mean: 8785820.88000  |  Std. Dev.: 14797913.90272  |  Coef. of Variance: 1.68429  |  Variance: 218978255872263.18750\n",
      "BBRv2 - Mean: 34885604.12000  |  Std. Dev.: 61778929.03139  |  Coef. of Variance: 1.77090  |  Variance: 3816636072264945.50000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "==========================================\n",
    "## bost-dtn (2021-06-16:02:12)\n",
    "### 1. bbr2_p1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "pscheduler_bbr2_p1  = os.path.join(rootdir, \"2021-06-16:02:12/pscheduler_bbr2_p1\")\n",
    "filenames = traverse(pscheduler_bbr2_p1)\n",
    "\n",
    "data_seg = []\n",
    "tput_bbr2_p1 = []\n",
    "key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        bbr2_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and j==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key2 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                bbr2_data_seg = data[j][key2]\n",
    "                data_seg.append( bbr2_data_seg )\n",
    "\n",
    "                throughput = (bbr2_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                tput_bbr2_p1.append( throughput )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p1):.5f}  |  Std. Dev.: {np.std(tput_bbr2_p1):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p1)/np.mean(tput_bbr2_p1)):.5f}  |  Variance: {np.var(tput_bbr2_p1):.5f}\")\n",
    "print(\"Data Segment\")\n",
    "print(f\"BBRv2 - Mean: {np.mean(data_seg):.5f}  |  Std. Dev.: {np.std(data_seg):.5f}  |  Coef. of Variance: {(np.std(data_seg)/np.mean(data_seg)):.5f}  |  Variance: {np.var(data_seg):.5f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 55\n",
      "Throughput\n",
      "BBRv2 - Mean: 6.33176  |  Std. Dev.: 2.85659  |  Coef. of Variance: 0.45115  |  Variance: 8.16012\n",
      "Data Segment\n",
      "BBRv2 - Mean: 11637878.67273  |  Std. Dev.: 7704520.51691  |  Coef. of Variance: 0.66202  |  Variance: 59359636395545.71094\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bost-dtn (2021-06-16:02:12)\n",
    "### 2. cubic_p1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "pscheduler_cubic_p1 = os.path.join(rootdir, \"2021-06-16:02:12/pscheduler_cubic_p1\")\n",
    "filenames = traverse(pscheduler_cubic_p1)\n",
    "\n",
    "data_seg = []\n",
    "tput_p1_cubic = []\n",
    "key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        # The MongoDB JSON dump has one object per line, so this works for me.\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        cubic_data_seg, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in enumerate(data):\n",
    "            if \"interval\" in d.keys() and j==0:\n",
    "                start_time = data[j]['interval']['time'] # Start time of the test\n",
    "            elif key1 in data[j].keys():\n",
    "                end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "\n",
    "                cubic_data_seg = data[j][key1]\n",
    "                data_seg.append( cubic_data_seg )\n",
    "\n",
    "                throughput = (cubic_data_seg*mss*8)/(end_time-start_time)/1e9\n",
    "                tput_p1_cubic.append( throughput )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"CUBIC - Mean: {np.mean(tput_p1_cubic):.5f}  |  Std. Dev.: {np.std(tput_p1_cubic):.5f}  |  Coef. of Variance: {(np.std(tput_p1_cubic)/np.mean(tput_p1_cubic)):.5f}  |  Variance: {np.var(tput_p1_cubic):.5f}\")\n",
    "print(\"Data Segment\")\n",
    "print(f\"CUBIC - Mean: {np.mean(data_seg):.5f}  |  Std. Dev.: {np.std(data_seg):.5f}  |  Coef. of Variance: {(np.std(data_seg)/np.mean(data_seg)):.5f}  |  Variance: {np.var(data_seg):.5f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 54\n",
      "Throughput\n",
      "CUBIC - Mean: 6.34994  |  Std. Dev.: 3.08753  |  Coef. of Variance: 0.48623  |  Variance: 9.53285\n",
      "Data Segment\n",
      "CUBIC - Mean: 11014398.94444  |  Std. Dev.: 7909237.57513  |  Coef. of Variance: 0.71808  |  Variance: 62556039019839.64844\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bost-dtn (2021-06-16:02:12)\n",
    "### 3. both_p16"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "pscheduler_both_p16 = os.path.join(rootdir, \"2021-06-16:02:12/pscheduler_both_p16\")\n",
    "filenames = traverse(pscheduler_both_p16)\n",
    "\n",
    "data_seg_sum_cubic, data_seg_sum_bbr2 = [], []\n",
    "tput_cubic_p16, tput_bbr2_p16 = [], []\n",
    "key1, key2, key3 = 'cubic_data_segs', 'bbr2_data_segs', 'bbr_data_segs'\n",
    "\n",
    "for i,f in enumerate(filenames):\n",
    "    try:\n",
    "        path = Path(f)\n",
    "        data = [json.loads(line) for line in open(f, 'r')]\n",
    "        \n",
    "        throughput_cubic, throughput_bbr2, throughput, mss, start_time, end_time = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        for j,d in zip(range(len(data)),data):\n",
    "            try:\n",
    "                if \"interval\" in d.keys() and j==0:\n",
    "                    start_time = d['interval']['time'] # Start time of the test\n",
    "                if \"streams\" in d.keys():\n",
    "                    end_time = data[j-1]['interval']['time'] # End time of the test\n",
    "                    mss = data[j-1]['interval']['mss'] # maximum segment size\n",
    "                    \n",
    "                    cubic_data_seg_list, bbr2_data_seg_list = [], []\n",
    "                    for j in range(len(d['streams'])):\n",
    "                        if \"cubic\" in d['streams'][j]['cc']:\n",
    "                            cubic_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                        elif \"bbr2\" in d['streams'][j]['cc']:\n",
    "                            bbr2_data_seg_list.append(d['streams'][j]['data_segs'])\n",
    "                    \n",
    "                    data_seg_sum_cubic.append( sum(cubic_data_seg_list) )\n",
    "                    throughput_cubic = (sum(cubic_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_cubic_p16.append( throughput_cubic )\n",
    "\n",
    "                    data_seg_sum_bbr2.append( sum(bbr2_data_seg_list) )\n",
    "                    throughput_bbr2 = (sum(bbr2_data_seg_list)*mss*8)/(end_time-start_time)/1e9\n",
    "                    tput_bbr2_p16.append( throughput_bbr2 )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Throughput\")\n",
    "print(f\"CUBIC - Mean: {np.mean(tput_cubic_p16):.5f}  |  Std. Dev.: {np.std(tput_cubic_p16):.5f}  |  Coef. of Variance: {(np.std(tput_cubic_p16)/np.mean(tput_cubic_p16)):.5f}  |  Variance: {np.var(tput_cubic_p16):.5f}\")\n",
    "print(f\"BBRv2 - Mean: {np.mean(tput_bbr2_p16):.5f}  |  Std. Dev.: {np.std(tput_bbr2_p16):.5f}  |  Coef. of Variance: {(np.std(tput_bbr2_p16)/np.mean(tput_bbr2_p16)):.5f}  |  Variance: {np.var(tput_bbr2_p16):.5f}\")\n",
    "\n",
    "print(\"Data Segment\")\n",
    "print(f\"CUBIC - Mean: {np.mean(data_seg_sum_cubic):.5f}  |  Std. Dev.: {np.std(data_seg_sum_cubic):.5f}  |  Coef. of Variance: {(np.std(data_seg_sum_cubic)/np.mean(data_seg_sum_cubic)):.5f}  |  Variance: {np.var(data_seg_sum_cubic):.5f}\")\n",
    "print(f\"BBRv2 - Mean: {np.mean(data_seg_sum_bbr2):.5f}  |  Std. Dev.: {np.std(data_seg_sum_bbr2):.5f}  |  Coef. of Variance: {(np.std(data_seg_sum_bbr2)/np.mean(data_seg_sum_bbr2)):.5f}  |  Variance: {np.var(data_seg_sum_bbr2):.5f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total files: 51\n",
      "Throughput\n",
      "CUBIC - Mean: 1.79498  |  Std. Dev.: 1.77901  |  Coef. of Variance: 0.99110  |  Variance: 3.16488\n",
      "BBRv2 - Mean: 6.21103  |  Std. Dev.: 1.99755  |  Coef. of Variance: 0.32161  |  Variance: 3.99021\n",
      "Data Segment\n",
      "CUBIC - Mean: 7093619.98039  |  Std. Dev.: 13211618.30685  |  Coef. of Variance: 1.86246  |  Variance: 174546858285969.28125\n",
      "BBRv2 - Mean: 34174089.37255  |  Std. Dev.: 51859948.61338  |  Coef. of Variance: 1.51752  |  Variance: 2689454270182731.00000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e443ef68753a57e21b26925269da7f3b0b849de1f4d00950fc0d90accc0b6012"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}